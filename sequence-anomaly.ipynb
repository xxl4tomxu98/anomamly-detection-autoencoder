{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a set of random string sequences that follow a specified format, and add a few anomalies. We're gonna start by writing a function that creates strings of the following format: CEBF0ZPQ ([4 letters A-F][1 digit 0–2][3 letters QWOPZXML]), and generate 25K sequences of this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "first_letters =  'ABCDEF'\n",
    "second_numbers = '120'\n",
    "last_letters = 'QWOPZXML'\n",
    "# returns a string of the following format: [4 letters A-F][1 digit 0-2][3 letters QWOPZXML]\n",
    "def get_random_string():\n",
    "    str1 = ''.join(random.choice(first_letters) for i in range(4))\n",
    "    str2 = random.choice(second_numbers)\n",
    "    str3 = ''.join(random.choice(last_letters) for i in range(3))\n",
    "    return str1+str2+str3\n",
    "# get 25,000 sequences of this format\n",
    "random_sequences = [get_random_string() for i in range(25000)]\n",
    "#this will return string according to the following format\n",
    "# ['CBCA2QOM', 'FBEF0WZW', 'DBFB2ZML', 'BFCB2WXO']\n",
    "# add some anomalies to our list\n",
    "random_sequences.extend(['XYDC2DCA', 'TXSX1ABC','RNIU4XRE','AABDXUEI','SDRAC5RF'])\n",
    "#save this to a dataframe\n",
    "seqs_ds = pd.DataFrame(random_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the string sequences into numbers and scale them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25005, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "#Build the char index that we will use to encode seqs to numbers \n",
    "#(this char index was written by Jason Brownlee from Machine Learning Mastery)\n",
    "char_index = '0abcdefghijklmnopqrstuvwxyz'\n",
    "char_index +='ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "char_index += '123456789'\n",
    "char_index += '().,-/+=&$?@#!*:;_[]|%⸏{}\\\"\\'' + ' ' +'\\\\'\n",
    "char_to_int = dict((c, i) for i, c in enumerate(char_index))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(char_index))\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "#function that convert a char seqs to numbers seqs \n",
    "#(it does a little more but lets leave it for now)\n",
    "def encode_sequence_list(seqs, feat_n=0):\n",
    "    encoded_seqs = []\n",
    "    for seq in seqs:\n",
    "        encoded_seq = [char_to_int[c] for c in seq]\n",
    "        encoded_seqs.append(encoded_seq)\n",
    "    if(feat_n > 0):\n",
    "        encoded_seqs.append(np.zeros(feat_n))\n",
    "    return pad_sequences(encoded_seqs, padding='post')\n",
    "\n",
    "def decode_sequence_list(seqs):\n",
    "    decoded_seqs = []\n",
    "    for seq in seqs:\n",
    "        decoded_seq = [int_to_char[i] for i in seq]\n",
    "        decoded_seqs.append(decoded_seq)\n",
    "    return decoded_seqs\n",
    "    \n",
    "# Using the char_index, the encode_sequence_list function\n",
    "# will turn a string like this EBCA0OXO \n",
    "#to an array like this [29 32 27 27  0 42 42 38]\n",
    "\n",
    "# encode each string seq to an integer array [[1],[5],[67]], [[45],[76],[7]\n",
    "encoded_seqs = encode_sequence_list(random_sequences)\n",
    "# mix everything up\n",
    "np.random.shuffle(encoded_seqs)\n",
    "encoded_seqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EAFE2OLL\n",
      "[27 28 32 31 53 43 49 49]\n"
     ]
    }
   ],
   "source": [
    "print(random_sequences[10])\n",
    "print(encoded_seqs[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have an array of the following shape as every string sequence has 8 characters, each of which is encoded as a number which we will treat as a column. Finally, before feeding the data to the autoencoder I'm going to scale the data using a MinMaxScaler, and split it into a training and test set. Proper scaling can often significantly improve the performance of NNs so it is important to experiment with more than one method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale our data using a MinMaxScaler that will scale \n",
    "#each number so that it will be between 0 and 1\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaled_seqs = scaler.fit_transform(encoded_seqs)\n",
    "#Create a test and train sets of our data\n",
    "X_train = scaled_seqs[:20000]\n",
    "X_test = scaled_seqs[20000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Design, fit and tune the autoencoder.\n",
    "As mentioned earlier, there is more than one way to design an autoencoder. It is usually based on small hidden layers wrapped with larger layers (this is what creates the encoding-decoding effect). I have made a few tuning sessions in order to determine the best params to use here as different kinds of data usually lend themselves to very different best-performance parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-29 12:06:47.376619: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-05-29 12:06:47.376747: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2022-05-29 12:06:47.739846: I tensorflow/core/profiler/lib/profiler_session.cc:126] Profiler session initializing.\n",
      "2022-05-29 12:06:47.739856: I tensorflow/core/profiler/lib/profiler_session.cc:141] Profiler session started.\n",
      "2022-05-29 12:06:47.740190: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session tear down.\n",
      "2022-05-29 12:06:47.906441: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-05-29 12:06:47.906607: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-29 12:06:54.289146: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 25/157 [===>..........................] - ETA: 0s - loss: 0.2933 - accuracy: 0.0468"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-29 12:06:56.255865: I tensorflow/core/profiler/lib/profiler_session.cc:126] Profiler session initializing.\n",
      "2022-05-29 12:06:56.255874: I tensorflow/core/profiler/lib/profiler_session.cc:141] Profiler session started.\n",
      "2022-05-29 12:06:56.263561: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2022-05-29 12:06:56.267199: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session tear down.\n",
      "2022-05-29 12:06:56.270863: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: /tmp/autoencoder/train/plugins/profile/2022_05_29_12_06_56\n",
      "2022-05-29 12:06:56.271746: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for trace.json.gz to /tmp/autoencoder/train/plugins/profile/2022_05_29_12_06_56/Toms-MacBook-Air.local.trace.json.gz\n",
      "2022-05-29 12:06:56.274087: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: /tmp/autoencoder/train/plugins/profile/2022_05_29_12_06_56\n",
      "2022-05-29 12:06:56.274237: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for memory_profile.json.gz to /tmp/autoencoder/train/plugins/profile/2022_05_29_12_06_56/Toms-MacBook-Air.local.memory_profile.json.gz\n",
      "2022-05-29 12:06:56.274629: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /tmp/autoencoder/train/plugins/profile/2022_05_29_12_06_56Dumped tool data for xplane.pb to /tmp/autoencoder/train/plugins/profile/2022_05_29_12_06_56/Toms-MacBook-Air.local.xplane.pb\n",
      "Dumped tool data for overview_page.pb to /tmp/autoencoder/train/plugins/profile/2022_05_29_12_06_56/Toms-MacBook-Air.local.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to /tmp/autoencoder/train/plugins/profile/2022_05_29_12_06_56/Toms-MacBook-Air.local.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to /tmp/autoencoder/train/plugins/profile/2022_05_29_12_06_56/Toms-MacBook-Air.local.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to /tmp/autoencoder/train/plugins/profile/2022_05_29_12_06_56/Toms-MacBook-Air.local.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 10s 9ms/step - loss: 0.2238 - accuracy: 0.0556 - val_loss: 0.0581 - val_accuracy: 0.0507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-29 12:06:57.551808: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 0.0481 - accuracy: 0.4326 - val_loss: 0.0386 - val_accuracy: 0.2188\n",
      "Epoch 3/30\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 0.0377 - accuracy: 0.2202 - val_loss: 0.0358 - val_accuracy: 0.2304\n",
      "Epoch 4/30\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 0.0325 - accuracy: 0.3423 - val_loss: 0.0226 - val_accuracy: 0.6420\n",
      "Epoch 5/30\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 0.0199 - accuracy: 0.6413 - val_loss: 0.0154 - val_accuracy: 0.6420\n",
      "Epoch 6/30\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 0.0151 - accuracy: 0.6363 - val_loss: 0.0142 - val_accuracy: 0.6420\n",
      "Epoch 7/30\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 0.0139 - accuracy: 0.6420 - val_loss: 0.0133 - val_accuracy: 0.6464\n",
      "Epoch 8/30\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 0.0131 - accuracy: 0.6457 - val_loss: 0.0124 - val_accuracy: 0.6671\n",
      "Epoch 9/30\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 0.0122 - accuracy: 0.6696 - val_loss: 0.0116 - val_accuracy: 0.6753\n",
      "Epoch 10/30\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 0.0115 - accuracy: 0.6707 - val_loss: 0.0111 - val_accuracy: 0.6829\n",
      "Epoch 11/30\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 0.0110 - accuracy: 0.6716 - val_loss: 0.0107 - val_accuracy: 0.6823\n",
      "Epoch 12/30\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 0.0105 - accuracy: 0.6788 - val_loss: 0.0103 - val_accuracy: 0.6839\n",
      "Epoch 13/30\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 0.0101 - accuracy: 0.6855 - val_loss: 0.0100 - val_accuracy: 0.6893\n",
      "Epoch 14/30\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 0.0099 - accuracy: 0.6785 - val_loss: 0.0096 - val_accuracy: 0.6913\n",
      "Epoch 15/30\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 0.0096 - accuracy: 0.6861 - val_loss: 0.0094 - val_accuracy: 0.6959\n",
      "Epoch 16/30\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 0.0093 - accuracy: 0.6943 - val_loss: 0.0091 - val_accuracy: 0.7001\n",
      "Epoch 17/30\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 0.0090 - accuracy: 0.6905 - val_loss: 0.0090 - val_accuracy: 0.6977\n",
      "Epoch 18/30\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 0.0091 - accuracy: 0.6895 - val_loss: 0.0089 - val_accuracy: 0.6951\n",
      "Epoch 19/30\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 0.0090 - accuracy: 0.6869 - val_loss: 0.0089 - val_accuracy: 0.6993\n",
      "Epoch 20/30\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 0.0089 - accuracy: 0.6875 - val_loss: 0.0088 - val_accuracy: 0.6977\n",
      "Epoch 21/30\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 0.0088 - accuracy: 0.6907 - val_loss: 0.0088 - val_accuracy: 0.6927\n",
      "Epoch 22/30\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 0.0088 - accuracy: 0.6908 - val_loss: 0.0087 - val_accuracy: 0.6923\n",
      "Epoch 23/30\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 0.0087 - accuracy: 0.6850 - val_loss: 0.0085 - val_accuracy: 0.6923\n",
      "Epoch 24/30\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 0.0083 - accuracy: 0.6832 - val_loss: 0.0080 - val_accuracy: 0.6971\n",
      "Epoch 25/30\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 0.0078 - accuracy: 0.6948 - val_loss: 0.0069 - val_accuracy: 0.7668\n",
      "Epoch 26/30\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 0.0064 - accuracy: 0.7751 - val_loss: 0.0052 - val_accuracy: 0.8112\n",
      "Epoch 27/30\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 0.0050 - accuracy: 0.8108 - val_loss: 0.0045 - val_accuracy: 0.8278\n",
      "Epoch 28/30\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 0.0044 - accuracy: 0.8225 - val_loss: 0.0041 - val_accuracy: 0.8398\n",
      "Epoch 29/30\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 0.0040 - accuracy: 0.8333 - val_loss: 0.0039 - val_accuracy: 0.8438\n",
      "Epoch 30/30\n",
      "157/157 [==============================] - 1s 8ms/step - loss: 0.0039 - accuracy: 0.8419 - val_loss: 0.0039 - val_accuracy: 0.8436\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import regularizers\n",
    "\n",
    "input_dim = X_train.shape[1] # the # features\n",
    "encoding_dim = 8 # first layer\n",
    "hidden_dim = int(encoding_dim / 2) #hideen layer\n",
    "\n",
    "nb_epoch = 30\n",
    "batch_size = 128\n",
    "learning_rate = 0.1\n",
    "\n",
    "input_layer = Input(shape=(input_dim, ))\n",
    "encoder = Dense(encoding_dim, activation=\"tanh\", activity_regularizer=regularizers.l1(10e-5))(input_layer)\n",
    "encoder = Dense(hidden_dim, activation=\"relu\")(encoder)\n",
    "decoder = Dense(encoding_dim, activation='relu')(encoder)\n",
    "decoder = Dense(input_dim, activation='tanh')(decoder)\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "#create callback\n",
    "filepath = 'my_best_model.hdf5'\n",
    "checkpointer = ModelCheckpoint(filepath=filepath,                             \n",
    "                             verbose=0, \n",
    "                             save_best_only=True)\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='/tmp/autoencoder', histogram_freq=0,\n",
    "                          write_graph=True,\n",
    "                          write_images=True)\n",
    "\n",
    "history = autoencoder.fit(X_train, X_train,\n",
    "                    epochs=nb_epoch,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(X_test, X_test),\n",
    "                    verbose=1,\n",
    "                    callbacks=[checkpointer, tensorboard]).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "plt.plot(history['loss'])\n",
    "plt.plot(history['val_loss'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Loss:0.003943485673516989\n"
     ]
    }
   ],
   "source": [
    "autoencoder = load_model(filepath)\n",
    "print(f\"Min Loss:{np.min(history['loss'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the Error and Find the Anomalies!\n",
    "Now, we feed the data again as a whole to the autoencoder and check the error term on each sample. Recall that seqs_ds is a pandas DataFrame that holds the actual string sequences. Line #2 encodes each string, and line #4 scales it. Then, I use the predict() method to get the reconstructed inputs of the strings stored in seqs_ds. Finally, I get the error term for each data point by calculating the “distance” between the input data point (or the actual data point) and the output that was reconstructed by the autoencoder:\n",
    "`mse = np.mean(np.power(actual_data - reconstructed_data, 2), axis=1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-29 12:15:10.277559: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CEEF0OPL</td>\n",
       "      <td>0.003283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADEB0XWO</td>\n",
       "      <td>0.002919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CFAA1XOW</td>\n",
       "      <td>0.004977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FEAE0PWW</td>\n",
       "      <td>0.005148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EFBE2XLP</td>\n",
       "      <td>0.003786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25000</th>\n",
       "      <td>XYDC2DCA</td>\n",
       "      <td>0.209306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25001</th>\n",
       "      <td>TXSX1ABC</td>\n",
       "      <td>0.356510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25002</th>\n",
       "      <td>RNIU4XRE</td>\n",
       "      <td>0.161482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25003</th>\n",
       "      <td>AABDXUEI</td>\n",
       "      <td>0.004716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25004</th>\n",
       "      <td>SDRAC5RF</td>\n",
       "      <td>0.147246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25005 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0       MSE\n",
       "0      CEEF0OPL  0.003283\n",
       "1      ADEB0XWO  0.002919\n",
       "2      CFAA1XOW  0.004977\n",
       "3      FEAE0PWW  0.005148\n",
       "4      EFBE2XLP  0.003786\n",
       "...         ...       ...\n",
       "25000  XYDC2DCA  0.209306\n",
       "25001  TXSX1ABC  0.356510\n",
       "25002  RNIU4XRE  0.161482\n",
       "25003  AABDXUEI  0.004716\n",
       "25004  SDRAC5RF  0.147246\n",
       "\n",
       "[25005 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#encode all the data\n",
    "encoded_seqs = encode_sequence_list(seqs_ds.iloc[:,0])\n",
    "#scale it\n",
    "scaled_data = MinMaxScaler().fit_transform(encoded_seqs)\n",
    "#predict it\n",
    "predicted = autoencoder.predict(scaled_data)\n",
    "#get the error term\n",
    "mse = np.mean(np.power(scaled_data - predicted, 2), axis=1)\n",
    "#now add them to our data frame\n",
    "seqs_ds['MSE'] = mse\n",
    "display(seqs_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we store the error term in the data frame, we can see how well each input data was constructed by our autoencoder.\n",
    "\n",
    "How do we find the anomalies?\n",
    "Well, the first thing we need to do is decide what is our threshold, and that usually depends on our data and domain knowledge. Some will say that an anomaly is a data point that has an error term that is higher than 95% of our data, for example. That would be an appropriate threshold if we expect that 5% of our data will be anomalous. However, recall that we injected 5 anomalies to a list of 25,000 perfectly formatted sequences, which means that only 0.02% of our data is anomalous, so we want to set our threshold as higher than 99.98% of our data (or the 0.9998 percentile). So first let's find this threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.9998 threshold:0.009507248666678644\n"
     ]
    }
   ],
   "source": [
    "mse_threshold = np.quantile(seqs_ds[\"MSE\"], 0.9998)\n",
    "print(f\"MSE 0.9998 threshold:{mse_threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I will add an MSE_Outlier column to the data set and set it to 1 when the error term crosses this threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs_ds['MSE_Outlier'] = 0\n",
    "seqs_ds.loc[seqs_ds['MSE'] > mse_threshold, 'MSE_Outlier'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now all we have to do is check how many outliers do we have and whether these outliers are the ones we injected and mixed in the data\n",
    "['XYDC2DCA', 'TXSX1ABC','RNIU4XRE','AABDXUEI','SDRAC5RF']. So let's see how many outliers we have and whether they are the ones we injected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUm of MSE Outlier:6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>MSE</th>\n",
       "      <th>MSE_Outlier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25000</th>\n",
       "      <td>XYDC2DCA</td>\n",
       "      <td>0.209306</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25001</th>\n",
       "      <td>TXSX1ABC</td>\n",
       "      <td>0.356510</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25002</th>\n",
       "      <td>RNIU4XRE</td>\n",
       "      <td>0.161482</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25003</th>\n",
       "      <td>AABDXUEI</td>\n",
       "      <td>0.004716</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25004</th>\n",
       "      <td>SDRAC5RF</td>\n",
       "      <td>0.147246</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0       MSE  MSE_Outlier\n",
       "25000  XYDC2DCA  0.209306            1\n",
       "25001  TXSX1ABC  0.356510            1\n",
       "25002  RNIU4XRE  0.161482            1\n",
       "25003  AABDXUEI  0.004716            0\n",
       "25004  SDRAC5RF  0.147246            1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"NUm of MSE Outlier:{seqs_ds['MSE_Outlier'].sum()}\")\n",
    "seqs_ds.iloc[25000:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('tf25')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "32b8582a6a317188b1b21a1addc55c3e93ba5302b85506ac8f3c8dfe71022194"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
